# 测试 AI（内测玩家）设计方案

## 目标

接入一个「测试 AI」，使其**扮演内测玩家**：自动进行多轮游戏、在适当时机结束测试，并产出**体验报告**与**优化建议**，供你直接查阅。

---

## 角色设定

| 维度 | 建议 |
|------|------|
| **人设** | 内测玩家：既会按正常流程体验（开局、移动、打听、修炼），也会故意尝试非常规操作（极端意图、重复指令、边界输入），以发现体验与逻辑问题。 |
| **目标** | 在有限轮次内感受：叙事是否连贯、世界是否自洽、时间/属性是否合理、是否有明显 bug 或空回复；结束时从「玩家视角」总结体验并给出可执行建议。 |

---

## 整体流程

```
┌─────────────────────────────────────────────────────────────────┐
│  1. 初始化：加载初始存档 / fixture，round = 0                     │
└─────────────────────────────────────────────────────────────────┘
                                    │
                                    ▼
┌─────────────────────────────────────────────────────────────────┐
│  2. 每轮循环                                                     │
│     · 将「当前对话摘要 + 当前状态 + 上轮建议动作」发给测试 AI      │
│     · 测试 AI 返回：下一句意图（或「结束测试」）                  │
│     · 若结束 → 跳转 3                                             │
│     · 否则：构建 payload → 调裁决 API → 更新存档与对话 → round++ │
│     · 若 round ≥ 最大轮次 → 强制结束，跳转 3                      │
└─────────────────────────────────────────────────────────────────┘
                                    │
                                    ▼
┌─────────────────────────────────────────────────────────────────┐
│  3. 生成报告                                                     │
│     · 将「完整对话记录 + 最终状态摘要」发给测试 AI                 │
│     · 测试 AI 返回：体验报告 + 优化建议（结构化或 Markdown）       │
│     · 与「主要聊天」一起写入 docs/playtest-reports/ 下带日期的报告  │
└─────────────────────────────────────────────────────────────────┘
```

---

## 输入 / 输出约定

### 每轮：测试 AI 的输入

给测试 AI 的上下文建议包含（可做截断以控制 token）：

- **最近 N 轮对话**：例如最近 5 轮，每轮「玩家：xxx」「系统：xxx」。
- **当前状态摘要**：当前年份/月份、所在地、体力/属性/资源（可精简为关键几项）、本局已进行轮数。
- **上轮系统建议动作**：`suggested_actions` 的 3 条，便于测试 AI 选择「跟建议走」或「故意不跟」。
- **可选**：本轮是否已超时/报错（若有则允许测试 AI 直接选择「结束测试」并说明原因）。

### 每轮：测试 AI 的输出（结构化，便于解析）

建议**单轮输出**为 JSON，便于脚本解析、避免歧义：

```json
{
  "action": "continue",
  "intent": "前往洛阳",
  "reason": "想测试跨区域移动与叙事是否连贯"
}
```

或结束测试：

```json
{
  "action": "end_test",
  "reason": "已体验约 10 轮，叙事与时间推进正常，准备写报告"
}
```

字段约定：

- `action`：`"continue"` | `"end_test"`。
- `intent`：当 `action === "continue"` 时必填，一句话玩家意图（会原样发给裁决 API）。
- `reason`：简短说明，便于你事后看「测试 AI 为什么这么选」。

若模型不擅长稳定输出 JSON，可退化为「固定格式的纯文本 + 正则/简单解析」（例如：`下一句：前往洛阳` / `结束测试`）。

### 报告阶段：测试 AI 的输入

- **完整对话记录**：所有「玩家：xxx」「系统：xxx」。
- **最终状态**：结束时的年份、地点、属性/资源、总轮数。
- **可选**：是否出现过超时/报错。

### 报告阶段：测试 AI 的输出

建议仍用**结构化**，便于写入报告文件并做后续分析：

```json
{
  "summary": "2～4 句话的体验总结：叙事是否连贯、世界是否自洽、有无明显问题。",
  "strengths": ["优点1", "优点2"],
  "issues": ["问题或 bug 1", "问题2"],
  "suggestions": ["建议1：可考虑…", "建议2：…"]
}
```

或允许输出一整段 **Markdown**（体验报告 + 建议），由脚本原样写入 `docs/playtest-reports/` 下当次报告的「体验报告与建议」章节。

---

## 技术实现思路

### 1. 测试 AI 的调用方式

- **与游戏裁决分离**：测试 AI 只负责「选意图」和「写报告」，不参与游戏内叙事生成；游戏侧仍只调现有裁决 API。
- **LLM 接入**：抽象为一层「调用任意 LLM」的接口（如 `callTestAI(messages): Promise<string>`），内部可用：
  - 同一项目的 DeepSeek/OpenAI 等 HTTP API；
  - 或你本地/内网的其它模型服务。
- **环境变量**：例如 `TEST_AI_API_URL`、`TEST_AI_API_KEY`（若需要），与 `ADJUDICATION_API` 分开配置，便于不同环境使用不同模型或 key。

### 2. Prompt 设计要点

- **系统提示（System）**：明确身份（内测玩家）、目标（体验流程 + 发现问题）、输出格式（JSON 或固定文本）、禁止事项（不要编造游戏里没有的地名/人名，意图需符合三国文字冒险）。
- **每轮用户提示（User）**：当前对话摘要 + 状态摘要 + 建议动作 + 本轮要求（请输出下一句意图或结束测试，并给出 reason）。
- **报告轮用户提示**：完整对话 + 最终状态 + 要求（请输出体验报告与建议，按给定 JSON 或 Markdown 格式）。

可在 `src/agents/` 或单独 `scripts/playtest/` 下维护「测试 AI」的 prompt 模板，与游戏内叙事 prompt 分离。

### 3. 与现有体验测试的衔接

- 在现有 `tests/playtestExperience.test.ts` 或单独脚本（如 `scripts/playtestWithAI.ts`）中：
  - 保留「构建 payload → 调裁决 API → 记录对话」的链路；
  - 将「下一句意图」从固定列表改为「调用测试 AI 得到 intent 或 end_test」；
  - 在循环结束后再调一次测试 AI，传入完整对话与状态，拿到报告与建议；
  - 最后输出：**主要聊天信息**（不变）+ **体验报告与建议**（来自测试 AI），一并写入 `docs/playtest-reports/` 下带日期的报告文件并打印。

这样你「跑一次自动化测试，就能拿到对话 + 体验报告 + 建议」的诉求可以满足。

### 4. 安全与成本

- **内容安全**：测试 AI 生成的 `intent` 会进入裁决 API，若使用同一套内容审核，可保持与正式玩家一致的风控。
- **成本**：测试 AI 每轮 1 次调用，报告阶段 1 次调用；可通过最大轮次上限（如 15～20 轮）和对话截断控制 token，避免单次测试过长。

---

## 建议的目录与职责

```
scripts/playtest/           # 可选：独立目录便于扩展
  prompts.ts                # 测试 AI 的 system / user 模板
  testAIClient.ts           # 调用 LLM，返回 JSON 或文本
  runner.ts                 # 主流程：循环 + 裁决 API + 报告生成
  types.ts                  # NextIntentResult, ExperienceReport 等
tests/
  playtestExperience.test.ts   # 现有：固定意图回归
  playtestWithAITest.ts        # 可选：集成「测试 AI」的 E2E（需配置 API）
```

或保持简洁：仅在 `tests/` 下增加一个「带测试 AI 的用例」，prompt 与调用封装在 `src/agents/playtestAgent.ts`（或同名脚本内）也可。

---

## 输出示例（最终 report 结构）

```markdown
# 游戏体验自动化测试报告

## 主要聊天信息
（与现有一致：每轮 玩家 / 系统）

## 体验报告与建议（由测试 AI 生成）

### 体验总结
（2～4 句话）

### 优点
- …

### 问题与 Bug
- …

### 优化建议
- …

---
共 N 轮，测试 AI 结束原因：…，执行时间：…
```

---

## 小结

| 项目 | 建议 |
|------|------|
| **测试 AI 角色** | 内测玩家：正常体验 + 适度非常规操作，目标发现体验与逻辑问题。 |
| **流程** | 每轮：测试 AI 选意图或结束 → 脚本调裁决 API 并记录 → 结束后测试 AI 写体验报告与建议。 |
| **输入** | 每轮：近期对话 + 状态摘要 + 建议动作；报告轮：完整对话 + 最终状态。 |
| **输出** | 每轮：JSON（action + intent + reason）；报告轮：JSON 或 Markdown（summary + issues + suggestions）。 |
| **接入** | 抽象 callTestAI，支持任意 LLM；prompt 独立维护；与现有 playtest 脚本/用例衔接，统一输出到 docs/playtest-reports/。 |

按此设计，你可以直接接入一个「测试 AI」代表内测玩家跑完整流程，并在每次测试结束后拿到**主要聊天 + 体验报告 + 建议**。

---

## 已落地实现

- **入口**：`npm run playtest`（需配置 `ADJUDICATION_API` 与 `DEEPSEEK_API_KEY` 或 `HUNYUAN_API_KEY`，与游戏云函数使用同一 Key 即可）。
- **每次运行**：测试 AI 自主进行一轮完整体验（约 8～15 轮），结束后在 `docs/playtest-reports/` 下生成带日期的报告文件。
- **由 Cursor/助手发起**：在对话中让助手执行「帮我跑一次体验测试」或直接运行 `npm run playtest` 即可；助手会在终端执行该命令，你只需事先在本地或环境变量中配置好上述两个变量。
